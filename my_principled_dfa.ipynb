{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my_principled_dfa",
      "provenance": [],
      "mount_file_id": "1SUXLuZW-VvCKTHF7E9NEdb5PAy0W4_qh",
      "authorship_tag": "ABX9TyMK3wiTnHwXjRVL5cDru5np",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaFred/principled_DFA/blob/main/my_principled_dfa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZJAfBSdtrlm",
        "outputId": "5b947100-22e7-4655-d8d7-cf7bcc91f6fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "% cd /content/drive/My Drive/principled-dfa/principled-dfa-training/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/principled-dfa/principled-dfa-training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4XbMHhIvPdN",
        "outputId": "b8967e03-8537-4fc8-8acb-562eec273d2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "! ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3_test_DFA_alignment.al  cifar-10-batches-py\t mltools\n",
            "3_test_DFA_angle.ang\t cifar-10-python.tar.gz  README.md\n",
            "best_practices.py\t dfatools\t\t save_path_bn\n",
            "bottleneck_from_log.py\t log_3_test_DFA.txt\t state_3_test_DFA.pt\n",
            "bottleneck.py\t\t log_retrieval.py\t training_log_3_test_DFA.tl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FyVQVxPu4bh"
      },
      "source": [
        "import argparse as ap\n",
        "import fnmatch as fnm\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle as pk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as opt\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "import dfatools.models as dfamodels\n",
        "import mltools.data as data\n",
        "import mltools.logging as log\n",
        "import mltools.models as models\n",
        "import mltools.processing as proc\n",
        "import mltools.utilities as util\n",
        "\n",
        "LOGTAG = \"MAIN\"\n",
        "\n",
        "\n",
        "# Utility functions.\n",
        "def remove_dropout(model_description):\n",
        "    new_model = model_description.copy()\n",
        "    for layer_name, layer in model_description.items():\n",
        "        if type(layer) == nn.Dropout or type(layer) == nn.Dropout2d or type(layer) == nn.Dropout3d:\n",
        "            del new_model[layer_name]\n",
        "    return new_model\n",
        "\n",
        "\n",
        "def remove_batchnorm(model_description):\n",
        "    new_model = model_description.copy()\n",
        "    for layer_name, layer in model_description.items():\n",
        "        if type(layer) == nn.BatchNorm1d or type(layer) == nn.BatchNorm2d or type(layer) == nn.BatchNorm3d:\n",
        "            del new_model[layer_name]\n",
        "    return new_model\n",
        "\n",
        "\n",
        "def run_once(data_path, save_path, test=False, use_dfa=True, use_feedback_normalization=True, use_PAI=False,\n",
        "             use_conv=False, dropout_rate=None, use_batchnorm=False, activation=nn.Tanh, dataset='CIFAR-10',\n",
        "             epochs=5, batch_size=128, past_state_path=None, seed=0, gpu_id=0):\n",
        "    base_name = f\"3_{'test' if test else 'eval'}_{'DFA' if use_dfa else 'BP'}\"\n",
        "    f\"_{'FBnorm' if use_feedback_normalization else 'nonorm'}\"\n",
        "    f\"_{'initPAI' if use_PAI else 'initSTD'}_{'CONV' if use_conv else 'FC'}\"\n",
        "    f\"_{'ND' if dropout_rate is None else f'e{dropout_rate[0]}_fc{dropout_rate[1]}_conv{dropout_rate[2]}'}\"\n",
        "    f\"_{'BN' if use_batchnorm else 'X'}_{activation().__str__()}_{dataset}_e{epochs}_bs{batch_size}\"\n",
        "    f\"_{'retrieved' if past_state_path is not None else 'from_scratch'}_s{seed}\"\n",
        "\n",
        "    # Set-up a logging file.\n",
        "\n",
        "    \n",
        "    if save_path is not None:\n",
        "        log_file_name = \"log_{0}.txt\".format(base_name)\n",
        "        log_save_path = os.path.join(save_path, log_file_name)\n",
        "        log.setup_logging(log.Level.INFO, log_save_path)\n",
        "    else:\n",
        "        log.setup_logging(log.Level.INFO)\n",
        "        log.log(\"You have not specified a save file, no data will be kept from the run!\", LOGTAG, log.Level.ERROR)\n",
        "\n",
        "    log.log(\"<b><u>Establishing Baselines for Direct Feedback Alignment</u></b>\", LOGTAG, log.Level.WARNING)\n",
        "    log.log(\"<b>Section 3 -- Establishing Best Practices for DFA</b>\", LOGTAG, log.Level.WARNING)\n",
        "\n",
        "    log.log(\"Setting-up processing back-end and seeds...\", LOGTAG, log.Level.INFO)\n",
        "    # For larger architectures that have high memory needs, the feedback matrix can be kept on another GPU (rp_device)\n",
        "    # and the BP model used for angle calculations as well (bp_device). Implementation is not fully tested for BP on\n",
        "    # a separate device, and some tensors may need to be moved around. This code has also not been tested on CPU only.\n",
        "    device = proc.enable_cuda(gpu_id, seed)\n",
        "    rp_device, bp_device = device, device\n",
        "\n",
        "    # Setting-up random number generation.\n",
        "    log.log(f\"Seeding with <b>{seed}</b>.\", LOGTAG, log.Level.DEBUG)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Setting-up data: transforms and data loaders.\n",
        "    log.log(f\"Preparing data for dataset <b>{dataset}</b> with batch size {batch_size}...\", LOGTAG, log.Level.INFO)\n",
        "\n",
        "    train_loader, validation_loader = data.get_loaders(dataset, batch_size, test, data_path)\n",
        "    if use_PAI:\n",
        "        # Prepare a separate loader for prealignment. This allows to experiment with different batch size and transforms\n",
        "        # on prealignment.\n",
        "        PAI_loader, _ = data.get_loaders(dataset, batch_size, test, data_path)\n",
        "\n",
        "    # Setting-up model.\n",
        "    log.log(\"Creating model...\", LOGTAG, log.Level.INFO)\n",
        "    # We create a model description with all the features possible (batchnorm and dropout) and then remove them if\n",
        "    # they are not required.\n",
        "    keep_dropout = True\n",
        "    if dropout_rate is None:\n",
        "        keep_dropout = False\n",
        "        dropout_rate = [0, 0, 0]\n",
        "    if use_conv:\n",
        "        log.log(\"Using a <b>convolutional</b> architecture.\", LOGTAG, log.Level.DEBUG)\n",
        "        model_description = OrderedDict([('drop1', nn.Dropout2d(dropout_rate[0])),\n",
        "                                         ('conv1', nn.Conv2d(3, 32, 5, padding=2)),\n",
        "                                         ('drop2', nn.Dropout2d(dropout_rate[2])),\n",
        "                                         ('act1', activation()),\n",
        "                                         ('bn1', nn.BatchNorm2d(32)),\n",
        "                                         ('maxp1', nn.MaxPool2d(3, 2)),\n",
        "                                         ('conv2', nn.Conv2d(32, 64, 5, padding=2)),\n",
        "                                         ('drop3', nn.Dropout2d(dropout_rate[2])),\n",
        "                                         ('act2', activation()),\n",
        "                                         ('bn2', nn.BatchNorm2d(64)),\n",
        "                                         ('maxp2', nn.MaxPool2d(3, 2)),\n",
        "                                         ('conv3', nn.Conv2d(64, 64, 5, padding=2)),\n",
        "                                         ('drop4', nn.Dropout2d(dropout_rate[2])),\n",
        "                                         ('act3', activation()),\n",
        "                                         ('bn3', nn.BatchNorm2d(64)),\n",
        "                                         ('maxp3', nn.MaxPool2d(3, 2)),\n",
        "                                         ('flat', util.Flatten()),\n",
        "                                         ('lin1', nn.Linear(576, 128)),\n",
        "                                         ('drop5', nn.Dropout(dropout_rate[1])),\n",
        "                                         ('act4', activation()),\n",
        "                                         ('bn4', nn.BatchNorm1d(128)),\n",
        "                                         ('lin2', nn.Linear(128, 10))])\n",
        "    else:\n",
        "        log.log(\"Using a <b>fully-connected</b> architecture.\", LOGTAG, log.Level.DEBUG)\n",
        "        model_description = OrderedDict([('flat', util.Flatten()),\n",
        "                                         ('drop1', nn.Dropout(dropout_rate[0])),\n",
        "                                         ('lin1', nn.Linear(3072, 800)),\n",
        "                                         ('drop2', nn.Dropout(dropout_rate[1])),\n",
        "                                         ('act1', activation()),\n",
        "                                         ('bn1', nn.BatchNorm1d(800)),\n",
        "                                         ('lin2', nn.Linear(800, 800)),\n",
        "                                         ('drop3', nn.Dropout(dropout_rate[1])),\n",
        "                                         ('act2', activation()),\n",
        "                                         ('bn2', nn.BatchNorm1d(800)),\n",
        "                                         ('lin3', nn.Linear(800, 800)),\n",
        "                                         ('drop4', nn.Dropout(dropout_rate[1])),\n",
        "                                         ('act3', activation()),\n",
        "                                         ('bn3', nn.BatchNorm1d(800)),\n",
        "                                         ('lin4', nn.Linear(800, 10))])\n",
        "\n",
        "    if not keep_dropout:\n",
        "        model_description = remove_dropout(model_description)\n",
        "    else:\n",
        "        log.log(f\"Using <b>dropout</b> with rate {dropout_rate[0]} for input, {dropout_rate[2]} for conv. layers,\"\n",
        "                      f\" {dropout_rate[2]} for fully-connected layers.\", LOGTAG, log.Level.DEBUG)\n",
        "    if not use_batchnorm:\n",
        "        model_description = remove_batchnorm(model_description)\n",
        "    else:\n",
        "        log.log(\"Using <b>batch normalization</b>.\", LOGTAG, log.Level.DEBUG)\n",
        "\n",
        "    if use_dfa or use_PAI:\n",
        "        # Create the DFA model, even if we run BP we will need it to calculate the prealignment initialization.\n",
        "        model_dfa = dfamodels.DFAClassifier(device, device, model_description, train_loader, validation_loader,\n",
        "                                            saving=(base_name, save_path, 5))\n",
        "        model_dfa = model_dfa.to(device)\n",
        "        if not use_feedback_normalization:\n",
        "            log.log(\"<b>Not</b> using feedback normalization.\", LOGTAG, log.Level.INFO)\n",
        "            model_dfa.model.feedback_normalization = False\n",
        "        model_dfa.initialize()\n",
        "        if use_PAI:\n",
        "            log.log(\"Using <b>PAI</b> (Pre-Alignment Initialization).\", LOGTAG, log.Level.DEBUG)\n",
        "            model_dfa.prealignment(PAI_loader, nn.CrossEntropyLoss())\n",
        "        if use_dfa:\n",
        "            log.log(\"Will be using <b>DFA</b> for training!\", LOGTAG, log.Level.WARNING)\n",
        "            model = model_dfa\n",
        "    if not use_dfa:\n",
        "        log.log(\"Will be using <b>BP</b> for training!\", LOGTAG, log.Level.WARNING)\n",
        "        model = models.Classifier(model_description, train_loader, validation_loader,\n",
        "                                  saving=(base_name, save_path, 5))\n",
        "        model = model.to(device)\n",
        "        if use_PAI:\n",
        "            model.model.load_state_dict(model_dfa.model.state_dict())\n",
        "\n",
        "    if past_state_path is not None:\n",
        "        with open(past_state_path, 'rb') as state_file:\n",
        "            model.model.load_state_dict(torch.load(state_file))\n",
        "\n",
        "    # Setting-up optimizer.\n",
        "    optimizer_description = (opt.SGD, {'lr': 5 * 1e-4})\n",
        "\n",
        "    # Train the model.\n",
        "    log.log(f\"Training with {'DFA' if use_dfa else 'BP'} initialized by {'PAI' if use_PAI else 'STD'} \"\n",
        "            f\"for {epochs} epochs on dataset {dataset} with a batch size of {batch_size}:\", LOGTAG, log.Level.INFO)\n",
        "    log.log(f\"Model: {model}\", LOGTAG, log.Level.DEBUG)\n",
        "    model.train(epochs, optimizer_description, loss_criterion=nn.CrossEntropyLoss())\n",
        "\n",
        "    # Validate the model one last time.\n",
        "    log.log(f\"Final validation with {'DFA' if use_dfa else 'BP'} initialized by {'PAI' if use_PAI else 'STD'} \"\n",
        "            f\"for {epochs} epochs on dataset {dataset} with a batch size of {batch_size}:\", LOGTAG, log.Level.INFO)\n",
        "    model.validate(loss_criterion=nn.CrossEntropyLoss())\n",
        "\n",
        "    # Save the model training log and the weights (message log and angles are already saved dynamically).\n",
        "    training_log_file_name = \"training_log_{0}.tl\".format(base_name)\n",
        "    training_log_file_path = os.path.join(save_path, training_log_file_name)\n",
        "    log.log(\"Finishing up: saving training log to {0}...\".format(training_log_file_path), LOGTAG, log.Level.INFO)\n",
        "    with open(training_log_file_path, 'wb') as training_log_file:\n",
        "        pk.dump(model.training_log, training_log_file)\n",
        "\n",
        "    state_dict_file_name = \"state_{0}.pt\".format(base_name)\n",
        "    state_dict_file_path = os.path.join(save_path, state_dict_file_name)\n",
        "    log.log(\"Finishing up: saving model state to {0}...\".format(state_dict_file_path), LOGTAG, log.Level.INFO)\n",
        "    torch.save(model.model.state_dict(), state_dict_file_path)\n",
        "\n",
        "    return model.training_log\n",
        "\n",
        "\n",
        "def process_training_log(path, training_log_name):\n",
        "    training_log_path = os.path.join(path, training_log_name)\n",
        "    with open(training_log_path, 'rb') as training_log_file:\n",
        "        training_log = pk.load(training_log_file)\n",
        "        train_error, test_error = [], []\n",
        "        for entry in training_log[:-1]:\n",
        "            if entry[0] == 'TRAIN':\n",
        "                train_error.append(entry[2]['topk'][0])\n",
        "            elif entry[0] == 'EVAL':\n",
        "                test_error.append(entry[2]['topk'][0])\n",
        "    return train_error, test_error\n",
        "\n",
        "def process_alignment_log(path, alignment_log_name):\n",
        "    alignment_log_path = os.path.join(path, alignment_log_name)\n",
        "    with open(alignment_log_path, 'rb') as alignment_log_file:\n",
        "        alignment_log = pk.load(alignment_log_file)\n",
        "        return alignment_log\n",
        "\n",
        "\n",
        "def analyse_results(base_name, results_path):\n",
        "    train_errors, test_errors = [], []\n",
        "    alignments = []\n",
        "    for file_name in os.listdir(results_path):\n",
        "        if fnm.fnmatch(file_name, '*.tl'):\n",
        "            if base_name in file_name:\n",
        "                run_train_error, run_test_error = process_training_log(results_path, file_name)\n",
        "                alignments.append(process_alignment_log(results_path, f\"{file_name[13:-3]}_alignment.al\")[-1])\n",
        "                train_errors.append(run_train_error[-1])\n",
        "                test_errors.append(run_test_error[-1])\n",
        "    print(base_name)\n",
        "    print('test error', np.mean(test_errors), np.std(test_errors))\n",
        "    print('train error', np.mean(train_errors), np.std(train_errors))\n",
        "    best_alignment = alignments[train_errors.index(max(train_errors))]\n",
        "    for layer in best_alignment:\n",
        "        print(layer[0], layer[1])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A9uJoYpvICX",
        "outputId": "7a4b3f48-0762-4e9d-ed28-175605881ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    # Setting-up arguments from command line.\n",
        "    parser = ap.ArgumentParser(description='Experimenting with prealignment initialization.')\n",
        "    parser.add_argument('-f') ## f: dummy parser to make the parse start\n",
        "    parser.add_argument('-test', action='store_true', default=True, help='use test set for validation.')\n",
        "    parser.add_argument('-g', '--gpu', type=int, default=0, help='ID of the GPU to use.')\n",
        "    parser.add_argument('-p', '--path', type=str, default='/data/mldata', help='path to folder containing datasets')\n",
        "    parser.add_argument('-r', '--savepath', type=str, default=None, help='path to save folder')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(torch.__version__)\n",
        "    run_number = 1\n",
        "\n",
        "    # TODO: Select where to save results for later processing.\n",
        "    data_path = \"/content/drive/My Drive/principled-dfa/principled-dfa-training/\"\n",
        "    save_path_bn = \"/content/drive/My Drive/principled-dfa/principled-dfa-training/save_path_bn\"\n",
        "    # save_path_bn = \"\"\n",
        "    # save_path_act = \"/content/drive/My Drive/principled-dfa/principled-dfa-training/save_path_act\"\n",
        "    # save_path_norm = \"/content/drive/My Drive/principled-dfa/principled-dfa-training/save_path_norm\"\n",
        "    # save_path_drop = \"/content/drive/My Drive/principled-dfa/principled-dfa-training/save_path_drop\"\n",
        "\n",
        "    half_leaky_relu = lambda: nn.LeakyReLU(0.5)\n",
        "    tenth_leaky_relu = lambda: nn.LeakyReLU(0.1)\n",
        "\n",
        "    for i in range(run_number):\n",
        "        # TODO: uncomment the lines for the experiments of interest.\n",
        "        # run_once(data_path, save_path_bn, test=True, use_conv=False, use_batchnorm=True, seed=i, gpu_id=0)\n",
        "        run_once(data_path, save_path_bn, test=True, use_conv=True, use_batchnorm=True, seed=i, gpu_id=0)\n",
        "        #run_once(data_path, save_path_act, test=True, activation=nn.Tanh, seed=i, gpu_id=2)\n",
        "        #run_once(data_path, save_path_act, test=True, activation=nn.ReLU, seed=i, gpu_id=3)\n",
        "        #run_once(data_path, save_path_act, test=True, activation=half_leaky_relu, seed=i, gpu_id=0)\n",
        "        #run_once(data_path, save_path_act, test=True, activation=tenth_leaky_relu, seed=i, gpu_id=1)\n",
        "        #run_once(data_path, save_path_act, test=True, activation=nn.LeakyReLU, seed=i, gpu_id=0)\n",
        "        #run_once(data_path, save_path_norm, test=True, use_feedback_normalization=False, seed=i, gpu_id=2)\n",
        "        #run_once(data_path, save_path_norm, test=True, use_conv=True, use_feedback_normalization=False, seed=i, gpu_id=3)\n",
        "        #run_once(data_path, save_path_drop, test=True, dropout_rate=[0.1, 0.5, 0.5], seed=i, gpu_id=0)\n",
        "        #run_once(data_path, save_path_drop, test=True, dropout_rate=[0.1, 0.1, 0.1], seed=i, gpu_id=1)\n",
        "        #run_once(data_path, save_path_drop, test=True, use_conv=True, dropout_rate=[0.1, 0.5, 0], seed=i, gpu_id=2)\n",
        "        #run_once(data_path, save_path_drop, test=True, use_conv=True, dropout_rate=[0.1, 0.1, 0], seed=i, gpu_id=3)\n",
        "        #run_once(data_path, save_path_bn, test=True, use_conv=True, seed=i, gpu_id=2)\n",
        "\n",
        "    # TODO: uncomment the lines for the processing of the experiment.\n",
        "    analyse_results('3_test_DFA_FBnorm_initSTD_FC_ND_BN_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_bn)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_CONV_ND_BN_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_bn)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_FC_ND_X_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_act)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_FC_ND_X_ReLU()_CIFAR-10_e50_bs128_from_scratch_s', save_path_act)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_FC_ND_X_LeakyReLU(negative_slope=0.5)_CIFAR-10_e50_bs128_from_scratch_s', save_path_act)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_FC_ND_X_LeakyReLU(negative_slope=0.1)_CIFAR-10_e50_bs128_from_scratch_s', save_path_act)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_FC_ND_X_LeakyReLU(negative_slope=0.01)_CIFAR-10_e50_bs128_from_scratch_s', save_path_act)\n",
        "    #analyse_results('3_test_DFA_nonorm_initSTD_FC_ND_X_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_norm)\n",
        "    #analyse_results('3_test_DFA_nonorm_initSTD_CONV_ND_X_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_norm)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_FC_e0.1_fc0.5_conv0.5_X_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_drop)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_FC_e0.1_fc0.1_conv0.1_X_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_drop)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_CONV_e0.1_fc0.5_conv0_X_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_drop)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_CONV_e0.1_fc0.1_conv0_X_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_drop)\n",
        "    #analyse_results('3_test_DFA_FBnorm_initSTD_CONV_ND_X_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_bn)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.6.0+cu101\n",
            "[LOG] \u001b[0m\u001b[36mINFO.\u001b[0m Will be writing a log of all messages in file /content/drive/My Drive/principled-dfa/principled-dfa-training/save_path_bn/log_3_test_DFA.txt.\u001b[0m\n",
            "\u001b[33m[MAIN] \u001b[0m\u001b[32m\u001b[4mWARN!\u001b[0m\u001b[33m \u001b[33m\u001b[1m\u001b[33m\u001b[4mEstablishing Baselines for Direct Feedback Alignment\u001b[0m\u001b[33m\u001b[0m\u001b[33m\u001b[0m\n",
            "\u001b[33m[MAIN] \u001b[0m\u001b[32m\u001b[4mWARN!\u001b[0m\u001b[33m \u001b[33m\u001b[1mSection 3 -- Establishing Best Practices for DFA\u001b[0m\u001b[33m\u001b[0m\n",
            "[MAIN] \u001b[0m\u001b[36mINFO.\u001b[0m Setting-up processing back-end and seeds...\u001b[0m\n",
            "[PROC] \u001b[0m\u001b[36mINFO.\u001b[0m Using GPU 0 with seed 0.\u001b[0m\n",
            "[MAIN] \u001b[0m\u001b[36mINFO.\u001b[0m Preparing data for dataset \u001b[1mCIFAR-10\u001b[0m with batch size 128...\u001b[0m\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[MAIN] \u001b[0m\u001b[36mINFO.\u001b[0m Creating model...\u001b[0m\n",
            "Sequential(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (act1): Tanh()\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxp1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (act2): Tanh()\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxp2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (act3): Tanh()\n",
            "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxp3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flat): Flatten()\n",
            "  (lin1): Linear(in_features=576, out_features=128, bias=True)\n",
            "  (act4): Tanh()\n",
            "  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lin2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "AsymmetricSequential(\n",
            "  (conv1): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (act1): Tanh()\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxp1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (ef1): AsymmetricFeedback()\n",
            "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (act2): Tanh()\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxp2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (ef2): AsymmetricFeedback()\n",
            "  (conv3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (act3): Tanh()\n",
            "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxp3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (flat): Flatten()\n",
            "  (ef0): AsymmetricFeedback()\n",
            "  (lin1): Linear(in_features=576, out_features=128, bias=True)\n",
            "  (act4): Tanh()\n",
            "  (bn4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (lin2): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n",
            "\u001b[33m[MAIN] \u001b[0m\u001b[32m\u001b[4mWARN!\u001b[0m\u001b[33m Will be using \u001b[33m\u001b[1mDFA\u001b[0m\u001b[33m for training!\u001b[0m\n",
            "[MAIN] \u001b[0m\u001b[36mINFO.\u001b[0m Training with DFA initialized by STD for 5 epochs on dataset CIFAR-10 with a batch size of 128:\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    EPOCH 1/5:\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 0_Tanh() -- mean:0.0026, std:0.0222\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 1_BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0023, std:0.0209\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 2_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0016, std:0.0165\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 3_Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:-0.0002, std:0.0116\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 4_Tanh() -- mean:0.0340, std:0.0438\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 5_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0258, std:0.0332\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 6_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0245, std:0.0289\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 7_Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:0.0208, std:0.0258\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 8_Tanh() -- mean:0.1412, std:0.1756\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 9_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0808, std:0.0636\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 10_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0879, std:0.0539\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 11_Flatten() -- mean:0.0545, std:0.0385\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 12_Linear(in_features=576, out_features=128, bias=True) -- mean:0.0545, std:0.0385\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 13_Tanh() -- mean:0.7063, std:0.1251\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 14_BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.9762, std:0.0155\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 15_Linear(in_features=128, out_features=10, bias=True) -- mean:0.9902, std:0.0094\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m Linear(in_features=128, out_features=10, bias=True) -- mean:0.9906, std:0.0093\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    Training loss: 0.014976322338080559 (top-1:32.3%) -- Validation loss: 0.013549840295314789 (top-1:39.3%).\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    EPOCH 2/5, last epoch processed in 17.38054323196411s:\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 0_Tanh() -- mean:0.0056, std:0.0190\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 1_BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0048, std:0.0173\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 2_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0022, std:0.0148\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 3_Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:0.0018, std:0.0109\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 4_Tanh() -- mean:0.0398, std:0.0492\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 5_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0337, std:0.0314\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 6_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0361, std:0.0324\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 7_Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:0.0307, std:0.0267\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 8_Tanh() -- mean:0.1932, std:0.1541\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 9_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.1327, std:0.0635\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 10_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.1334, std:0.0537\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 11_Flatten() -- mean:0.0791, std:0.0415\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 12_Linear(in_features=576, out_features=128, bias=True) -- mean:0.0791, std:0.0415\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 13_Tanh() -- mean:0.7153, std:0.1221\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 14_BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.9745, std:0.0353\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 15_Linear(in_features=128, out_features=10, bias=True) -- mean:0.9951, std:0.0047\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m Linear(in_features=128, out_features=10, bias=True) -- mean:0.9953, std:0.0045\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    Training loss: 0.012399478273418469 (top-1:46.1%) -- Validation loss: 0.012024532961845398 (top-1:48.3%).\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    EPOCH 3/5, last epoch processed in 20.513529300689697s:\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 0_Tanh() -- mean:0.0060, std:0.0207\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 1_BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0051, std:0.0185\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 2_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0032, std:0.0167\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 3_Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:0.0017, std:0.0111\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 4_Tanh() -- mean:0.0510, std:0.0633\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 5_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0282, std:0.0360\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 6_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0330, std:0.0323\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 7_Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:0.0265, std:0.0258\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 8_Tanh() -- mean:0.1202, std:0.1253\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 9_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.1434, std:0.0610\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 10_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.1405, std:0.0568\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 11_Flatten() -- mean:0.0978, std:0.0453\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 12_Linear(in_features=576, out_features=128, bias=True) -- mean:0.0978, std:0.0453\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 13_Tanh() -- mean:0.5631, std:0.1420\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 14_BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.9670, std:0.0393\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 15_Linear(in_features=128, out_features=10, bias=True) -- mean:0.9904, std:0.0115\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m Linear(in_features=128, out_features=10, bias=True) -- mean:0.9907, std:0.0108\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    Training loss: 0.011506316841890414 (top-1:50.3%) -- Validation loss: 0.011871335697174072 (top-1:48.7%).\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    EPOCH 4/5, last epoch processed in 20.261334896087646s:\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 0_Tanh() -- mean:-0.0006, std:0.0195\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 1_BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:-0.0008, std:0.0175\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 2_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:-0.0010, std:0.0157\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 3_Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:-0.0002, std:0.0118\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 4_Tanh() -- mean:0.0356, std:0.0553\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 5_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0258, std:0.0327\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 6_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0262, std:0.0328\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 7_Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:0.0193, std:0.0258\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 8_Tanh() -- mean:0.1161, std:0.1564\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 9_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.1417, std:0.0741\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 10_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.1426, std:0.0629\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 11_Flatten() -- mean:0.1010, std:0.0438\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 12_Linear(in_features=576, out_features=128, bias=True) -- mean:0.1010, std:0.0438\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 13_Tanh() -- mean:0.5107, std:0.1725\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 14_BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.9554, std:0.0858\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 15_Linear(in_features=128, out_features=10, bias=True) -- mean:0.9874, std:0.0125\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m Linear(in_features=128, out_features=10, bias=True) -- mean:0.9882, std:0.0110\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    Training loss: 0.011017824413302617 (top-1:52.3%) -- Validation loss: 0.011266711962223053 (top-1:51.1%).\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    EPOCH 5/5, last epoch processed in 19.840623378753662s:\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 0_Tanh() -- mean:0.0060, std:0.0201\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 1_BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0061, std:0.0185\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 2_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0052, std:0.0162\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 3_Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:0.0036, std:0.0120\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 4_Tanh() -- mean:0.0351, std:0.0668\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 5_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.0250, std:0.0350\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 6_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.0275, std:0.0339\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 7_Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) -- mean:0.0245, std:0.0269\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 8_Tanh() -- mean:0.1378, std:0.1847\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 9_BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.1477, std:0.0653\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 10_MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False) -- mean:0.1531, std:0.0594\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 11_Flatten() -- mean:0.1123, std:0.0483\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 12_Linear(in_features=576, out_features=128, bias=True) -- mean:0.1123, std:0.0483\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 13_Tanh() -- mean:0.5243, std:0.1703\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 14_BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) -- mean:0.9616, std:0.0694\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m 15_Linear(in_features=128, out_features=10, bias=True) -- mean:0.9892, std:0.0159\u001b[0m\n",
            "[DMOD] \u001b[0m\u001b[36mINFO.\u001b[0m Linear(in_features=128, out_features=10, bias=True) -- mean:0.9903, std:0.0144\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m    Training loss: 0.010761214620791949 (top-1:53.1%) -- Validation loss: 0.010821539545059204 (top-1:53.1%).\u001b[0m\n",
            "[MODL] \u001b[0m\u001b[36mINFO.\u001b[0m Checkpoint! saving model state to /content/drive/My Drive/principled-dfa/principled-dfa-training/save_path_bn/state_3_test_DFA.pt...\u001b[0m\n",
            "[MAIN] \u001b[0m\u001b[36mINFO.\u001b[0m Final validation with DFA initialized by STD for 5 epochs on dataset CIFAR-10 with a batch size of 128:\u001b[0m\n",
            "[MAIN] \u001b[0m\u001b[36mINFO.\u001b[0m Finishing up: saving training log to /content/drive/My Drive/principled-dfa/principled-dfa-training/save_path_bn/training_log_3_test_DFA.tl...\u001b[0m\n",
            "[MAIN] \u001b[0m\u001b[36mINFO.\u001b[0m Finishing up: saving model state to /content/drive/My Drive/principled-dfa/principled-dfa-training/save_path_bn/state_3_test_DFA.pt...\u001b[0m\n",
            "3_test_DFA_FBnorm_initSTD_FC_ND_BN_Tanh()_CIFAR-10_e50_bs128_from_scratch_s\n",
            "test error nan nan\n",
            "train error nan nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:217: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
            "  keepdims=keepdims)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:186: RuntimeWarning: invalid value encountered in true_divide\n",
            "  arrmean, rcount, out=arrmean, casting='unsafe', subok=False)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:209: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-8b70a669580d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# TODO: uncomment the lines for the processing of the experiment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0manalyse_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'3_test_DFA_FBnorm_initSTD_FC_ND_BN_Tanh()_CIFAR-10_e50_bs128_from_scratch_s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path_bn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;31m#analyse_results('3_test_DFA_FBnorm_initSTD_CONV_ND_BN_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_bn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m#analyse_results('3_test_DFA_FBnorm_initSTD_FC_ND_X_Tanh()_CIFAR-10_e50_bs128_from_scratch_s', save_path_act)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-dffc520bd7b5>\u001b[0m in \u001b[0;36manalyse_results\u001b[0;34m(base_name, results_path)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0mbest_alignment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malignments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_alignment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRzN8z5HwhUE"
      },
      "source": [
        "%tb"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}